{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title: \n",
    "Store Sales - Time Series Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This project focuses on time series forecasting to predict store sales for Corporation Favorita, a large Ecuadorian-based grocery retailer. The objective is to build a model that accurately predicts the unit sales for thousands of items sold at different Favorita stores.\n",
    "\n",
    "\n",
    "### 1.1. Objectives\n",
    "Understand the data: The first objective is to gain insights into the store sales data, including store-specific information, product families, promotions, and sales numbers. This understanding will enable the company to make informed business decisions.\n",
    "\n",
    "Predict store sales: Develop a reliable time series forecasting model that accurately predicts the unit sales for different product families at various Favorita stores. This will help the company optimize inventory management, plan promotions, and improve overall sales performance.\n",
    "\n",
    "### 1.2. Methodology\n",
    "To achieve the objectives, we will follow a structured approach:\n",
    "\n",
    "Data Exploration: Thoroughly explore the provided datasets to understand the available features, their distributions, and relationships. This step will provide initial insights into the store sales data and help identify any data quality issues.\n",
    "\n",
    "Data Preparation: Handle missing values, perform feature engineering, and encode categorical variables as necessary. This step may involve techniques like imputation, scaling, and one-hot encoding.\n",
    "\n",
    "Time Series Analysis: Analyze the temporal aspects of the data, including trends, seasonality, and potential outliers. This analysis will provide a deeper understanding of the underlying patterns in store sales over time.\n",
    "\n",
    "Model Selection and Training: Select appropriate time series forecasting models and train them using the prepared data. Consider incorporating external factors like promotions, holidays, and oil prices, if available, to enhance the forecasting accuracy.\n",
    "\n",
    "Model Evaluation: Evaluate the trained models using appropriate metrics, such as mean absolute error (MAE), root mean squared error (RMSE), or mean absolute percentage error (MAPE). Assess the models' performance and identify the most accurate and reliable forecasting model.\n",
    "\n",
    "Model Deployment and Forecasting: Deploy the chosen model to predict store sales for future time periods, leveraging the provided test dataset. Generate forecasts for the target period and assess the model's ability to capture the sales patterns accurately.\n",
    "\n",
    "By following this methodology, we aim to provide valuable insights to the telecom company and develop a reliable predictive model for customer churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Statistical Analysis\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
    "\n",
    "\n",
    "# Feature Processing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Modelling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Other Packages\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data from SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file into a dictionary\n",
    "environment_vars = dotenv_values('.env')\n",
    "\n",
    "# Get the credential values from the '.env' file \n",
    "server = environment_vars.get(\"SERVER\")\n",
    "database = environment_vars.get(\"DATABASE\")\n",
    "username = environment_vars.get(\"USERNAME\")\n",
    "password = environment_vars.get(\"PASSWORD\")\n",
    "\n",
    "# Create the connection string using the ODBC driver format\n",
    "conn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "\n",
    "# Establish the connection using the connection string\n",
    "conn = pyodbc.connect(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the first table 'oil' in the database\n",
    "query1 = 'SELECT * FROM dbo.oil'\n",
    "oil_df = pd.read_sql(query1, conn)\n",
    "\n",
    "# Query the second table 'holidays_events' in the database\n",
    "query2 = 'SELECT * FROM dbo.holidays_events'\n",
    "holidays_events_df = pd.read_sql(query2, conn)\n",
    "\n",
    "# Query the third table 'stores' in the database\n",
    "query3 = 'SELECT * FROM dbo.stores'\n",
    "stores_df = pd.read_sql(query3, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>93.139999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>92.970001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>93.120003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>93.199997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  dcoilwtico\n",
       "0  2013-01-01         NaN\n",
       "1  2013-01-02   93.139999\n",
       "2  2013-01-03   92.970001\n",
       "3  2013-01-04   93.120003\n",
       "4  2013-01-07   93.199997"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dataframe\n",
    "oil_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>locale</th>\n",
       "      <th>locale_name</th>\n",
       "      <th>description</th>\n",
       "      <th>transferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-03-02</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Manta</td>\n",
       "      <td>Fundacion de Manta</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Regional</td>\n",
       "      <td>Cotopaxi</td>\n",
       "      <td>Provincializacion de Cotopaxi</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-04-12</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Cuenca</td>\n",
       "      <td>Fundacion de Cuenca</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-04-14</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Libertad</td>\n",
       "      <td>Cantonizacion de Libertad</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-04-21</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Riobamba</td>\n",
       "      <td>Cantonizacion de Riobamba</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     type    locale locale_name                    description  \\\n",
       "0  2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
       "1  2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
       "2  2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
       "3  2012-04-14  Holiday     Local    Libertad      Cantonizacion de Libertad   \n",
       "4  2012-04-21  Holiday     Local    Riobamba      Cantonizacion de Riobamba   \n",
       "\n",
       "   transferred  \n",
       "0        False  \n",
       "1        False  \n",
       "2        False  \n",
       "3        False  \n",
       "4        False  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dataframe\n",
    "holidays_events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr           city                           state type  cluster\n",
       "0          1          Quito                       Pichincha    D       13\n",
       "1          2          Quito                       Pichincha    D       13\n",
       "2          3          Quito                       Pichincha    D        8\n",
       "3          4          Quito                       Pichincha    D        9\n",
       "4          5  Santo Domingo  Santo Domingo de los Tsachilas    D        4"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dataframe\n",
    "stores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Train and Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Transactions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"transactions.csv\")\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions and Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "The promotional activities, oil prices, and holidays/events have a significant impact on store sales for Corporation Favorita.\n",
    "\n",
    "Null Hypothesis (H0): The promotional activities, oil prices, and holidays/events do not have a significant impact on store sales for Corporation Favorita.\n",
    "\n",
    "Alternative Hypothesis (H1): The promotional activities, oil prices, and holidays/events have a significant impact on store sales for Corporation Favorita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. Is the train dataset complete, including all the required dates?\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "3. Did the earthquake have an impact on sales?\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "5. Are sales affected by promotions, oil prices, and holidays?\n",
    "6. What analysis can be derived from the date and its extractable features?\n",
    "7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Analytical Questions:\n",
    "\n",
    "1. How does the presence of promotional activities affect store sales?\n",
    "\n",
    "2. What is the relationship between oil prices and store sales?\n",
    "\n",
    "3. Do holidays and events influence store sales? Are there specific holidays/events that drive higher sales?\n",
    "\n",
    "4. How does the historical sales trend vary across different stores and product families?\n",
    "\n",
    "    a. Are there certain product families types that exhibit higher sales performance?\n",
    "    b. Are there any notable correlations between variables such as store size, location, or cluster, and sales performance?\n",
    "\n",
    "5. Is there any seasonality or trend (recurring patterns) in the store sales data?\n",
    "\n",
    "6. Can we identify any specific time periods or events that led to significant changes in store sales?\n",
    "\n",
    "7. How does store transaction volume (number of transactions) correlate with sales? Are there any insights to be gained from transaction data?\n",
    "\n",
    "9. Does store sales vary based on the day of the week or month of the year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA), Data Preprocessing & Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An in-depth exploration of the datasets is presented to gain insights into the available variables,their distributions and relationships. This step will provide an initial undertanding of the datasets to identify any data quality issues that will inform the cleaning and pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Shape of The Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the shapes of the train and test datasets\n",
    "print(f\"Train Dataset: {train_df.shape}\")\n",
    "print(f\"Test Datasets: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The train dataset contains 3,000,888 rows and 6 columns while the test dataset contains 28,512 rows and 5 columns.\n",
    "\n",
    "The train dataset is significantly larger than the test dataset in terms of the number of rows. This is expected, as the train dataset is usually larger to provide sufficient data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the shapes of the other datasets\n",
    "print(\"Shapes of Each Dataset:\")\n",
    "print(f\"Holiday Events Dataset: {holidays_events_df.shape}\")\n",
    "print(f\"Oil Dataset: {oil_df.shape}\")\n",
    "print(f\"Stores Dataset: {stores_df.shape}\")\n",
    "print(f\"Transactions Dataset: {transactions_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Holiday Events dataset contains 350 rows and 6 columns. This dataset provides information about various holidays and events.\n",
    "\n",
    "The Oil dataset consists of 1,218 rows and 2 columns. This dataset includes information about the daily price of oil.\n",
    "\n",
    "The Stores dataset contains 54 rows and 5 columns. This dataset provides details about different stores, such as their locations, types, and clusters.\n",
    "\n",
    "The Transactions dataset contains 83,488 rows and 3 columns. This dataset contains information about the number of transactions made at each store on specific dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Column Information of The Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the column information of each dataset\n",
    "# Function to display column information of the datasets\n",
    "def show_column_info(dataset_name, dataset):\n",
    "    print(f\"Data types for the {dataset_name} dataset:\")\n",
    "    print(dataset.info())\n",
    "    print('==='*14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column information of the train and test columns\n",
    "show_column_info('Train', train_df)\n",
    "print()\n",
    "show_column_info('Test', test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The train dataset contains 3,000,888 entries and 6 columns: 'id', 'date', 'store_nbr', 'family', 'sales', and 'onpromotion'.\n",
    "- The test dataset contains 28,512 entries and 5 columns: 'id', 'date', 'store_nbr', 'family', and 'onpromotion'.\n",
    "- The \"date\" column in both datasets of type object. It needs to be converted to a datetime data type for further analysis.\n",
    "- As expected, the test dataset does not have the \"sales\" column. This column is not needed because 'sales' is the variable we want to predict. The goal is to use the trained model to predict or forecast the sales in the test data based on the other available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column information of the other datasets\n",
    "show_column_info('Holiday events', holidays_events_df)\n",
    "print()\n",
    "show_column_info('Oil', oil_df)\n",
    "print()\n",
    "show_column_info('Stores', stores_df)\n",
    "print()\n",
    "show_column_info('Transactions', transactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Holiday Events Dataset:\n",
    "- The dataset contains 350 entries and 6 columns: 'date', 'type', 'locale', 'locale_name', 'description', and 'transferred'.\n",
    "- The \"date\" column in the dataset is of type object. It needs to be converted to a datetime data type for further analysis.\n",
    "\n",
    "##### The Oil Dataset:\n",
    "- The dataset contains 1,218 entries has 2 columns: 'date' and 'dcoilwtico'.\n",
    "- The \"date\" column in the dataset is of type object. It needs to be converted to a datetime data type for further analysis.\n",
    "- The 'dcoilwtico' column has 1,175 non-null values, indicating that there are some missing values in this column.\n",
    "\n",
    "##### The Stores dataset:\n",
    "- The dataset contains 54 entries and 5 columns: 'store_nbr', 'city', 'state', 'type', and 'cluster'.\n",
    "\n",
    "##### The Transactions dataset:\n",
    "- The dataset contains 83,488 entries and 3 columns: 'date', 'store_nbr', and 'transactions'.\n",
    "- The \"date\" column in the dataset is of type object. It needs to be converted to a datetime data type for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Transforming the 'date' column to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the 'date' column in the datasets to datetime format\n",
    "# Train dataset\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "\n",
    "# Test dataset\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "# Holiday Events dataset\n",
    "holidays_events_df['date'] = pd.to_datetime(holidays_events_df['date'])\n",
    "\n",
    "# Oil dataset\n",
    "oil_df['date'] = pd.to_datetime(oil_df['date'])\n",
    "\n",
    "# Transactions dataset\n",
    "transactions_df['date'] = pd.to_datetime(transactions_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the data type of the 'date' column after transformation\n",
    "print('Date Column Data Type After Transformation:') \n",
    "print('==='*14)\n",
    "print(\"Train dataset:\", train_df['date'].dtype)\n",
    "print(\"Test dataset:\", test_df['date'].dtype)\n",
    "print(\"Holiday Events dataset:\", holidays_events_df['date'].dtype)\n",
    "print(\"Oil dataset:\", oil_df['date'].dtype)\n",
    "print(\"Transactions dataset:\", transactions_df['date'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'date' columns in all datasets are formatted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Summary statistics of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking for the summary statistics of the datasets \n",
    "datasets = {'train': train_df, 'test': test_df, 'holiday events': holidays_events_df, 'oil': oil_df, 'stores': stores_df, 'transactions': transactions_df}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    print(f\"{name.capitalize()} dataset summary statistics:\")\n",
    "    print('---'*15)\n",
    "    print(data.describe())\n",
    "    print('==='*20)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Checking for Missing Values in The Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the datasets\n",
    "datasets = {'train': train_df, 'test': test_df, 'holiday events': holidays_events_df, 'oil': oil_df, 'stores': stores_df, 'transactions': transactions_df, }\n",
    "\n",
    "def show_missing_values(datasets):\n",
    "    for name, data in datasets.items():\n",
    "        print(f\"Missing values in the {name.capitalize()} dataset:\")\n",
    "        print(data.isnull().sum())\n",
    "        print('===' * 18)\n",
    "        print()\n",
    "\n",
    "show_missing_values(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the columns in the different datasets, it is only the 'dcoilwtico' (daily crude oil prices) column in the Oil dataset which has missing values. The column has 43 missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the Missing Values in the 'dcoilwtico' (daily crude oil prices) column of the Oil Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the 'dcoilwtico' column to Identify a Strategy for Handling Missing Values\n",
    "fig = px.line(oil_df, x='date', y='dcoilwtico')\n",
    "fig.update_layout(title='Trend of Oil Prices Over Time', title_x=0.5, xaxis_title='Date', yaxis_title='Oil Price')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill missing values in the 'dcoilwtico' column using backfill strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'dcoilwtico' column using backfill strategy\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].fillna(method='backfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values after handling\n",
    "missing_values_after = oil_df['dcoilwtico'].isnull().sum()\n",
    "missing_values_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the 'dcoilwtico' column to confirm proper handling of Missing Values\n",
    "fig = px.line(oil_df, x='date', y='dcoilwtico')\n",
    "fig.update_layout(title='Trend of Oil Prices Over Time', title_x=0.5, xaxis_title='Date', yaxis_title='Oil Price')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values in the 'dcoilwitco' column have been handled and are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### vi. Checking for the completeness of the 'date' column in the Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the completeness of the train dataset\n",
    "min_date = train_df['date'].min()\n",
    "max_date = train_df['date'].max()\n",
    "expected_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "missing_dates = expected_dates[~expected_dates.isin(train_df['date'])]\n",
    "\n",
    "if len(missing_dates) == 0:\n",
    "    print(\"The train dataset is complete. It includes all the required dates.\")\n",
    "else:\n",
    "    print(\"The train dataset is incomplete. The following dates are missing:\")\n",
    "    print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the missing dates in the train dataset\n",
    "# Create an index of the missing dates as a DatetimeIndex object\n",
    "missing_dates = pd.Index(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], dtype='datetime64[ns]')\n",
    "\n",
    "# Create a DataFrame with the missing dates, using the 'date' column\n",
    "missing_data = pd.DataFrame({'date': missing_dates})\n",
    "\n",
    "# Concatenate the original train dataset and the missing data DataFrame\n",
    "# ignore_index=True ensures a new index is assigned to the resulting DataFrame\n",
    "train_df = pd.concat([train_df, missing_data], ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame based on the 'date' column in ascending order\n",
    "train_df.sort_values('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm if the train dataset is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the completeness of the train dataset\n",
    "min_date = train_df['date'].min()\n",
    "max_date = train_df['date'].max()\n",
    "expected_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "missing_dates = expected_dates[~expected_dates.isin(train_df['date'])]\n",
    "\n",
    "if len(missing_dates) == 0:\n",
    "    print(\"The train dataset is complete. It includes all the required dates.\")\n",
    "else:\n",
    "    print(\"The train dataset is incomplete. The following dates are missing:\")\n",
    "    print(missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing dates have been handled successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi. Merging The Train Dataset with the Stores, Transactions, Holiday Events and Oil Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the common columns ('store_nbr' and 'date') in the datasets using the inner merge() function\n",
    "# Merge train_df with stores_df based on 'store_nbr' column\n",
    "merged_df1 = train_df.merge(stores_df, on='store_nbr', how='inner')\n",
    "\n",
    "# Merge merged_df1 with transactions_df based on 'date' and 'store_nbr' columns\n",
    "merged_df2 = merged_df1.merge(transactions_df, on=['date', 'store_nbr'], how='inner')\n",
    "\n",
    "# Merge merged_df2 with holidays_events_df based on 'date' column\n",
    "merged_df3 = merged_df2.merge(holidays_events_df, on='date', how='inner')\n",
    "\n",
    "# Merge merged_df3 with oil_df based on 'date' column\n",
    "merged_df = merged_df3.merge(oil_df, on='date', how='inner')\n",
    "\n",
    "# View the first five rows of the merged dataset\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of an inner merge in this time series forecasting project for corporation Favorita helps to ensure data consistency, avoid missing values, and focus on the relevant data for accurate predictions.\n",
    "\n",
    "An inner merge type retains only the rows with matching values in the specified columns. In the context of time series forecasting, it allows us to merge datasets based on a common time index or timestamp. By performing an inner merge, we ensure that only the rows with corresponding timestamps in both datasets are included in the merged result. This is important for time series forecasting because you want to align the data from different sources based on their timestamps to build a consistent and accurate forecasting model.\n",
    "\n",
    "With an inner merge, you eliminate any non-matching timestamps, which may not be useful for forecasting and could introduce inconsistencies in the data. By focusing on the intersection of the datasets, we can create a merged dataset that contains the necessary information for accurate time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column information of the merged dataset\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merged dataset after merging the train dataset with additional datasets contains 322,047 rows and 16 columns.\n",
    "Two columns have been renamed as a result of the merging, type_x and type_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values of the two unknown columns\n",
    "print(\"Unique values of 'type_x':\")\n",
    "print(merged_df['type_x'].unique())\n",
    "print()\n",
    "print(\"Unique values of 'type_y':\")\n",
    "print(merged_df['type_y'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merged dataset consists of 322,047 non-null observations.\n",
    "Two columns have been renamed as a result of the merging, type_x and type_y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The type_x column represents the store type.\n",
    "- The type_y column represents the holiday type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns with the approapriate names\n",
    "merged_df = merged_df.rename(columns={\"type_x\": \"store_type\", \"type_y\": \"holiday_type\"})\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics and transpose the rows and columns of the resultant DataFrame.\n",
    "# Transposing flips the DataFrame (the rows become columns and the columns become rows) for better readability\n",
    "merged_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viii. Checking for Missing Values in The Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the merged datasets\n",
    "missing_values = merged_df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merged dataset has no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ix. Checking for Duplicate Values in The Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate values in the merged dataset\n",
    "duplicate_rows_merged = merged_df.duplicated()\n",
    "duplicate_rows_merged.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate values in the test dataset\n",
    "duplicate_rows_test = test_df.duplicated()\n",
    "duplicate_rows_test.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates in the merged and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x. Save the merged dataset in a new CSV file to be used in PowerBI Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('Visualization_Data.csv', index=False)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A Copy of The Merged Dataset\n",
    "merged_df_copy = merged_df.copy()\n",
    "merged_df_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate, Multivariate and Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Distribution of the 'sales' variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(merged_df['sales'], bins=20)\n",
    "plt.xlabel('Sales')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sales')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot(merged_df['sales'])\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Boxplot of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram and boxplot of the 'sales' variable provide insights into its distribution. The histogram shows the frequency distribution of sales values. It reveals that the majority of sales fall within a specific range, represented by the peak in the histogram. However, there are also instances of higher sales values, leading to a right-skewed distribution. This skewness suggests that there are relatively fewer occurrences of very high sales, while the majority of sales are concentrated around lower values. The boxplot further confirms the presence of outliers in the data, as indicated by the points beyond the whiskers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. distribution of the 'transactions' variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(merged_df['transactions'], bins=20)\n",
    "plt.xlabel('Transactions')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Transactions')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot(merged_df['transactions'])\n",
    "plt.ylabel('Transactions')\n",
    "plt.title('Boxplot of Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram provides insights into the distribution of transactions in the dataset. The shape of the histogram indicates that the majority of transactions fall into a specific range, which is evident from the high frequency observed on the left side of the histogram. As the transactions increase, the frequency gradually decreases, forming a right-skewed distribution. This suggests that there are relatively fewer instances of high transaction volumes. Overall, the histogram highlights the presence of a cluster of transactions with a lower frequency, indicating a pattern in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Distribution of the 'Daily Oil Price' variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(merged_df['dcoilwtico'], bins=20)\n",
    "plt.xlabel('Oil Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Oil Price')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot(merged_df['dcoilwtico'])\n",
    "plt.ylabel('Oil Price')\n",
    "plt.title('Boxplot of Oil Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram and boxplot of the 'dcoilwtico' variable reveal insights into its distribution. The histogram displays the frequency distribution of oil prices, indicating the number of occurrences for each price range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Trend of sales over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group the data by date and calculate the total sales\n",
    "daily_sales = merged_df.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# Create a time series plot with slider\n",
    "fig = px.line(daily_sales, x='date', y='sales')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.update_layout(title='Trend of Sales Over Time', title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the trend of sales over time. From the graph, we can observe that the sales exhibit some variations and fluctuations over time. There are periods of both high and low sales, indicating potential seasonality or other factors affecting sales patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Trend of Daily Crude oil Prices Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the 'dcoilwtico' column to confirm if the trend is consistent.\n",
    "fig = px.line(oil_df, x='date', y='dcoilwtico')\n",
    "fig.update_layout(title='Trend of Oil Prices Over Time', title_x=0.5, xaxis_title='Date', yaxis_title='Oil Price')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a trend in the in the oil prices over time. We see that oil prices suffered a collapse towards the end of 2014 and have not recovered. In fact despite some volatility, oil prices are at the same level as they were in the beginning of 2015. As a result of this we may see a significant shift in store sales around late 2014. Looking at the unit sales data, this is not readily apparent. Although sales do appear to drop off in the early part of 2015, in late 2014 they are rising. Also, to add oil price drop doesn't seems to have any impact on the sales, as it was seen from the sales plot, there was no relation between oil price dropping on the sales, so we can say that this feature or data is of no importance to us and will not be considered during modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Total Count of Sales by Store Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the color palette to \"viridis\"\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total count and total sales per store type\n",
    "store_type_counts = merged_df['store_type'].value_counts()\n",
    "store_type_sales = merged_df.groupby('store_type')['sales'].sum()\n",
    "\n",
    "# Create a bar plot with \"viridis\" color palette for total count\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=store_type_counts.index, y=store_type_counts.values)\n",
    "plt.xlabel('Store Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Total Count by Store Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of the total count of sales by store type provides insights into the sales performance and customer demand across different store types. Store Type D stands out with the highest count of sales, suggesting a strong customer base and popularity of products offered. Store Type C follows with a relatively lower count of sales, indicating a significant customer base as well. On the other hand, Store Types A, B, and E have lower counts, suggesting potential areas for improvement or the need to address competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Total Amount in Sales by Store Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the store types by total sales\n",
    "store_type_sales = store_type_sales.sort_values(ascending=False)\n",
    "\n",
    "# Create a bar plot with \"viridis\" color palette for total sales\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.barplot(x=store_type_sales.index, y=store_type_sales.values, order=store_type_sales.index, palette=\"viridis\")\n",
    "plt.xlabel('Store Type')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.title('Total Sales by Store Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total sales amount varies across different store types. Store Type D has the highest total sales, indicating its significant contribution to the overall sales. Store Type A follows closely behind, demonstrating its substantial sales performance. Store Type C ranks third in terms of total sales, while Store Type B and Store Type E have lower sales amounts. Understanding the variations in sales by store type helps identify the key drivers of revenue and highlights the importance of certain store types in driving overall sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Average Sales by City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by city and calculate the average sales\n",
    "average_sales_by_city = merged_df.groupby('city')['sales'].mean()\n",
    "\n",
    "# Sort the data by average sales in ascending order\n",
    "average_sales_by_city = average_sales_by_city.sort_values(ascending=True)\n",
    "\n",
    "# Define colors for the bar plot using 'viridis' color palette\n",
    "colors = cm.viridis(np.linspace(0, 1, len(average_sales_by_city)))\n",
    "\n",
    "# Plot the average sales by city horizontally\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.barh(average_sales_by_city.index, average_sales_by_city.values, color=colors)\n",
    "plt.xlabel('Average Sales')\n",
    "plt.ylabel('City')\n",
    "plt.title('Average Sales by City')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quito has the highest number of stores, significantly surpassing other cities. Cayambe is the second most populated city in terms of stores followed by Ambato, Daule and Loja. Some cities have a moderate number of stores while others have a lower number of stores. Puyo has the lowest number of sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f. Average Sales by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by state and calculate the average sales\n",
    "average_sales_by_state = merged_df.groupby('state')['sales'].mean()\n",
    "\n",
    "# Sort the data by average sales in descending order\n",
    "average_sales_by_state = average_sales_by_state.sort_values(ascending=True)\n",
    "\n",
    "# Plot the average sales by state\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.barh(average_sales_by_state.index, average_sales_by_state.values, color=colors)\n",
    "plt.xlabel('Average Sales')\n",
    "plt.ylabel('State')\n",
    "plt.title('Average Sales by State')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pichincha has the highest number of stores, primarily due to the presence of Quito, the capital city. Guayas is the second most populous state in terms of stores, with Guayaquil being a major city. Santo Domingo de los Tsachilas, Azuay, Manabi, Cotopaxi, Tungurahua, Los Rios, El Oro, Chimborazo, Imbabura, Bolivar, Pastaza, Santa Elena, and Loja have a moderate number of stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g. Relationship between sales and transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='transactions', y='sales', data=merged_df)\n",
    "plt.xlabel('Transactions')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Relationship between Sales and Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot visualizes the relationship between sales and transactions in the dataset. Each data point represents a specific instance with corresponding sales and transaction values. Here are the key insights drawn from the scatter plot:\n",
    "\n",
    "Clustered Data Points: The majority of data points cluster in the lower sales region, forming a specific concentration. This clustering suggests that there are certain transaction volumes that are consistently associated with particular sales levels. This concentration may indicate a common sales pattern or trend that occurs at specific transaction levels.\n",
    "\n",
    "Outliers: Several data points deviate from the main cluster and are located at higher sales levels for relatively lower transaction volumes or vice versa. These outliers represent exceptional instances where sales are significantly different from what is typically observed for a given number of transactions. Identifying and understanding these outliers can provide valuable insights into unusual sales scenarios or exceptional business activities.\n",
    "\n",
    "In summary, the scatter plot provides valuable insights into the relationship between sales and transactions. The clustering of data points around specific sales and transaction levels indicates the presence of common patterns. Additionally, outliers represent exceptional cases that warrant further investigation to understand the factors influencing sales and transactions in unique instances. This analysis can help businesses make informed decisions and devise effective strategies to improve sales performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Correlation matrix of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical variables for correlation analysis\n",
    "numerical_vars = ['sales', 'transactions', 'dcoilwtico']\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = merged_df[numerical_vars].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation values range from -1 to 1, where -1 represents a perfect negative correlation, 1 represents a perfect positive correlation, and 0 represents no correlation. The table helps us understand how the variables are related to each other, providing valuable insights into their interactions. In this correlation matrix, we can see the correlations between different variables:\n",
    "\n",
    "- Sales and Transactions:\n",
    "There is a weak positive correlation of approximately 0.200 between \"Sales\" and \"Transactions.\" This suggests that there is a slight positive relationship between the number of transactions and the sales. It implies that when the number of transactions increases, there is a tendency for sales to increase as well, although the correlation is not very strong.\n",
    "\n",
    "- Sales and Dcoilwito (Oil Prices):\n",
    "There is a weak negative correlation of approximately -0.062 between \"Sales\" and \"Dcoilwito\" (Oil Prices). This indicates a slight negative relationship between sales and oil prices. It suggests that as oil prices increase, there is a tendency for sales to decrease slightly, though the correlation is not significant.\n",
    "\n",
    "- Transactions and Dcoilwito (Oil Prices):\n",
    "There is a very weak negative correlation of approximately -0.017 between \"Transactions\" and \"Dcoilwito\" (Oil Prices). This suggests that there is almost no relationship between the number of transactions and oil prices. It indicates that fluctuations in oil prices do not have a significant impact on the number of transactions.\n",
    "\n",
    "Overall, the correlation values are relatively low, indicating that the relationships between these variables are not very strong. Other factors not considered in this correlation matrix may also influence sales, transactions, and oil prices. It's essential to explore additional factors to gain a more comprehensive understanding of their impact on sales and transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Scatter Plot Marrix of numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical variables for correlation analysis\n",
    "numerical_vars = ['sales', 'transactions', 'dcoilwtico']\n",
    "\n",
    "# Plot scatter plot matrix\n",
    "sns.pairplot(merged_df[numerical_vars])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations of the scatter plot matrix corroborate the observations from the correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stationarity implies that the statistical properties of the time series, such as mean and variance, remain constant over time. In this case, the ADF test was conducted on the 'sales' data from the 'merged_df' dataset. To perform the stationarity test, we will use the Augmented Dickey-Fuller (ADF) test commonly used to check for stationarity in a time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Null hypothesis (H0): The sales data is non-stationary.\n",
    "- Alternative hypothesis (H1): The sales data is stationary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Test of the 'sales' column in the merged_df using Adfuller\n",
    "sales_data = merged_df['sales']\n",
    "\n",
    "# Perform ADF test\n",
    "result = adfuller(sales_data)\n",
    "\n",
    "# Extract the test statistics and p-value from the result\n",
    "test_statistic = result[0]\n",
    "p_value = result[1]\n",
    "critical_values = result[4]\n",
    "\n",
    "# Print the test statistics and critical values\n",
    "print(f\"ADF Test Statistics: {test_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(\"Critical Values:\")\n",
    "for key, value in critical_values.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Check the p-value against a significance level (e.g., 0.05)\n",
    "if p_value <= 0.05:\n",
    "    print(\"Reject the null hypothesis: The sales data is stationary.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The sales data is non-stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the ADF test, the test statistics (-43.83) is significantly lower than the critical values at all confidence levels (1%, 5%, and 10%). Additionally, the p-value is 0.0, which is lower than the significance level of 0.05.\n",
    "\n",
    "Since the p-value is less than 0.05, we reject the null hypothesis, indicating that the sales data is stationary. The test results suggest that the 'sales' column exhibits stationarity, which means the data has a constant mean and variance over time. This property is essential for time-series analysis and modeling, as it helps to ensure reliable forecasting and prediction of future sales trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing and Answering Key Analytical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis (H0): The promotional activities have a significant impact on store sales for Corporation Favorita.\n",
    "\n",
    "Alternative Hypothesis (H1): The promotional activities have a significant impact on store sales for Corporation Favorita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant variables for the hypothesis test\n",
    "promo_sales = merged_df[merged_df['onpromotion'] == 1]['sales']\n",
    "non_promo_sales = merged_df[merged_df['onpromotion'] == 0]['sales']\n",
    "\n",
    "# Perform a two-sample t-test to compare sales between promotional and non-promotional periods\n",
    "t_statistic, p_value = ttest_ind(promo_sales, non_promo_sales)\n",
    "\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Extract the relevant variables for the hypothesis test\n",
    "promo_sales = merged_df[merged_df['onpromotion'] == 1]['sales']\n",
    "non_promo_sales = merged_df[merged_df['onpromotion'] == 0]['sales']\n",
    "\n",
    "# Perform a two-sample t-test to compare sales between promotional and non-promotional periods\n",
    "t_statistic, p_value = ttest_ind(promo_sales, non_promo_sales)\n",
    "\n",
    "# Print the test result\n",
    "print(\"Hypothesis Testing for Promotional Activities:\")\n",
    "print(\"Null Hypothesis (H0): The promotional activities do not have a significant impact on store sales.\")\n",
    "print(\"Alternative Hypothesis (H1): The promotional activities have a significant impact on store sales.\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Test Statistic:\", t_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"=\" * 50)\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis. Promotional activities have a significant impact on store sales at Corporation Favorita.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. Promotional activities do not have a significant impact on store sales at Corporation Favorita.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the hypothesis test, we obtained a very low p-value of 0.0. This indicates strong evidence to reject the null hypothesis. Therefore, we can conclude that promotional activities have a significant impact on store sales for Corporation Favorita. The test statistic of 68.22 also suggests a substantial difference in sales between promotional and non-promotional periods. These results support the notion that promotional activities play a crucial role in driving store sales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Is the train dataset complete (has all the required dates)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the completeness of the train dataset\n",
    "min_date = train_df['date'].min()\n",
    "max_date = train_df['date'].max()\n",
    "expected_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "missing_dates = expected_dates[~expected_dates.isin(train_df['date'])]\n",
    "\n",
    "if len(missing_dates) == 0:\n",
    "    print(\"The train dataset is complete. It includes all the required dates.\")\n",
    "else:\n",
    "    print(\"The train dataset is incomplete. The following dates are missing:\")\n",
    "    print(missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Which dates have the lowest and highest sales for each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "merged_df['year'] = merged_df['date'].dt.year\n",
    "\n",
    "lowest_sales_dates = merged_df.groupby('year')['date'].min()\n",
    "highest_sales_dates = merged_df.groupby('year')['date'].max()\n",
    "\n",
    "print(\"Dates with the lowest sales for each year:\\n\", lowest_sales_dates)\n",
    "print(\"=\"*50)\n",
    "print(\"Dates with the highest sales for each year:\\n\", highest_sales_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Analyze the impact of the earthquake on sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the variable earthquake_date to the date the earthquake took place (April 16, 2016)\n",
    "earthquake_date = pd.to_datetime('2016-04-16')\n",
    "\n",
    "# Filter the sales data before and after the earthquake\n",
    "sales_before_earthquake = train_df[train_df['date'] < earthquake_date]['sales']\n",
    "sales_after_earthquake = train_df[train_df['date'] > earthquake_date]['sales']\n",
    "\n",
    "# Set the colormap to viridis\n",
    "colormap = cm.get_cmap('viridis')\n",
    "\n",
    "# Plot the sales before and after the earthquake\n",
    "plt.plot(sales_before_earthquake, color=colormap(0.2), label='Sales Before Earthquake')\n",
    "plt.plot(sales_after_earthquake, color=colormap(0.8), label='Sales After Earthquake')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a surge in sales after the eathquake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Determine if certain groups of stores sell more products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and calculate the average sales\n",
    "average_sales_by_cluster = merged_df.groupby('cluster')['sales'].mean()\n",
    "\n",
    "# Group by city and calculate the average sales\n",
    "average_sales_by_city = merged_df.groupby('city')['sales'].mean()\n",
    "\n",
    "# Group by state and calculate the average sales\n",
    "average_sales_by_state = merged_df.groupby('state')['sales'].mean()\n",
    "\n",
    "# Group by store type and calculate the average sales\n",
    "average_sales_by_store_type = merged_df.groupby('store_type')['sales'].mean()\n",
    "\n",
    "# Set the number of bars in each plot\n",
    "num_bars = len(average_sales_by_cluster)\n",
    "\n",
    "# Generate the colors using the viridis palette\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_bars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by average sales in descending order\n",
    "average_sales_by_cluster = average_sales_by_cluster.sort_values(ascending=False)\n",
    "\n",
    "# Plot the average sales by cluster\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(average_sales_by_cluster.index, average_sales_by_cluster.values, color=colors)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.title('Average Sales by Cluster')\n",
    "\n",
    "# Set the x-tick labels as integers\n",
    "plt.xticks(range(1, len(average_sales_by_cluster.index) + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster with the highest number of stores is Cluster 5, followed by Clusters 14, 8, 11 and 12. These clusters have a significantly larger number of stores compared to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by average sales in descending order\n",
    "average_sales_by_city = average_sales_by_city.sort_values(ascending=True)\n",
    "\n",
    "# Plot the average sales by city horizontally\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.barh(average_sales_by_city.index, average_sales_by_city.values, color=colors)\n",
    "plt.xlabel('Average Sales')\n",
    "plt.ylabel('City')\n",
    "plt.title('Average Sales by City')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quito has the highest number of stores, significantly surpassing other cities. Cayambe is the second most populated city in terms of stores followed by Ambato, Daule and Loja. Some cities have a moderate number of stores while others have a lower number of stores. Puyo has the lowest number of sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by average sales in descending order\n",
    "average_sales_by_state = average_sales_by_state.sort_values(ascending=True)\n",
    "\n",
    "# Plot the average sales by state\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(average_sales_by_state.index, average_sales_by_state.values, color=colors)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.title('Average Sales by State')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pichincha has the highest number of stores, primarily due to the presence of Quito, the capital city.\n",
    "Guayas is the second most populous state in terms of stores, with Guayaquil being a major city.\n",
    "Santo Domingo de los Tsachilas, Azuay, Manabi, Cotopaxi, Tungurahua, Los Rios, El Oro, Chimborazo, Imbabura, Bolivar, Pastaza, Santa Elena, and Loja have a moderate number of stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average sales by store type\n",
    "plt.bar(average_sales_by_store_type.index, average_sales_by_store_type.values, color=colors)\n",
    "plt.xlabel('Store Type')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.title('Average Sales by Store Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The store type with the highest average sales is type A followed by Type D. Store Type C has the lowest average sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Are sales affected by promotions, oil prices and holidays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between sales and promotions, oil prices, holidays\n",
    "corr_sales_promotions = merged_df['sales'].corr(merged_df['onpromotion'])\n",
    "corr_sales_oil = merged_df['sales'].corr(merged_df['dcoilwtico'])\n",
    "corr_sales_holidays = merged_df['sales'].corr(merged_df['holiday_type'] == 'Holiday')\n",
    "\n",
    "# Print the correlation values\n",
    "print(f\"Correlation between Sales and Promotions: {corr_sales_promotions}\")\n",
    "print(f\"Correlation between Sales and Oil Prices: {corr_sales_oil}\")\n",
    "print(f\"Correlation between Sales and Holidays: {corr_sales_holidays}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Promotions:\n",
    "There is a positive correlation of approximately 0.42 between sales and promotions. This suggests that promotions have a moderate positive impact on sales. When promotions are running, there is an increased likelihood of higher sales.\n",
    "\n",
    "\n",
    "- Oil Prices:\n",
    "There is a weak negative correlation of approximately -0.06 between sales and oil prices. This indicates that there is a slight negative relationship between sales and oil prices. However, the correlation is close to zero, suggesting that oil prices have minimal impact on sales.\n",
    "\n",
    "\n",
    "- Holidays:\n",
    "There is a very weak negative correlation of approximately -0.04 between sales and holidays. This indicates that there is almost no relationship between sales and holidays. Holidays do not seem to have a significant impact on sales.\n",
    "These insights suggest that promotions have a relatively stronger influence on sales compared to oil prices and holidays. While promotions positively impact sales, oil prices and holidays show minimal or no relationship with sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What analysis can we get from the date and its extractable features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Date Components\n",
    "merged_df_copy['date'] = pd.to_datetime(merged_df_copy['date'])\n",
    "merged_df_copy['year'] = merged_df_copy['date'].dt.year\n",
    "merged_df_copy['month'] = merged_df_copy['date'].dt.month\n",
    "merged_df_copy['day'] = merged_df_copy['date'].dt.day\n",
    "merged_df_copy['dayofweek'] = merged_df_copy['date'].dt.weekday\n",
    "merged_df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. How does the presence of promotional activities affect store sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for sales under different promotion conditions\n",
    "sales_with_promotion = train_df[train_df['onpromotion'] == 1]['sales']\n",
    "sales_without_promotion = train_df[train_df['onpromotion'] == 0]['sales']\n",
    "\n",
    "# Create a line plot to compare sales with and without promotions over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=sales_with_promotion, label='With Promotion')\n",
    "sns.lineplot(data=sales_without_promotion, label='Without Promotion')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Effect of Promotions on Store Sales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the relationship between oil prices and store sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a scatter plot to visualize the relationship between oil prices and store sales\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(merged_df['dcoilwtico'], merged_df['sales'])\n",
    "# plt.xlabel('Oil Prices')\n",
    "# plt.ylabel('Sales')\n",
    "# plt.title('Relationship between Oil Prices and Store Sales')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Do holidays and events influence store sales? Are there specific holidays/events that drive higher sales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. How does the historical sales trend vary across different stores and product families?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Are there certain product families types that exhibit higher sales performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by product family and calculate the total sales\n",
    "family_sales = merged_df.groupby('family')['sales'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Select the top 10 product families\n",
    "top_10_families = family_sales.head(10)\n",
    "\n",
    "# Plot the relationship between product family and sales for the top 10 families\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_10_families.index, y=top_10_families.values, palette='viridis')\n",
    "plt.xlabel('Product Family')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.title('Total Sales by Product Family (Top 10)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph illustrates the sales performance of the top 10 product families. Grocery I and beverages exhibit the highest sales, indicating their popularity among customers. Produce and cleaning products also demonstrate significant sales, reflecting the importance of fresh produce and household cleaning supplies. Dairy, bread/bakery, poultry, and meats contribute to overall sales, suggesting the demand for essential food items. Personal care and deli products have relatively lower sales but still play a role in the product mix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Are there any notable correlations between variables such as store number, location, or cluster, and sales performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Is there any seasonality or trend (recurring patterns) in the store sales data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot to visualize the store sales over time\n",
    "plt.plot(merged_df['date'], merged_df['sales'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Store Sales Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Can we identify any specific time periods or events that led to significant changes in store sales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. How does store transaction volume (number of transactions) correlate with sales? Are there any insights to be gained from transaction data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Does store sales vary based on the day of the week or month of the year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. How does the sales trend vary across different store numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales = merged_df.groupby('store_nbr')['sales'].sum()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(store_sales.index, store_sales.values, color=cm.viridis(store_sales.values/max(store_sales.values)))\n",
    "plt.xlabel('Store Number')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.title('Sales by Store Number')\n",
    "\n",
    "# Set the X-axis limits and ticks\n",
    "plt.xlim(0, 54)\n",
    "plt.xticks(range(55))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enginering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Date Components (Day, Month, Year and Day of The Week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Date Components\n",
    "merged_df_copy['date'] = pd.to_datetime(merged_df_copy['date'])\n",
    "merged_df_copy['year'] = merged_df_copy['date'].dt.year\n",
    "merged_df_copy['month'] = merged_df_copy['date'].dt.month\n",
    "merged_df_copy['day'] = merged_df_copy['date'].dt.day\n",
    "merged_df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Unneccessary Columns in The Merged and Test Datasets as it is not needed for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['date','id', 'locale', 'locale_name', 'description', 'store_type', 'transferred', 'state']\n",
    "merged_df_copy = merged_df_copy.drop(columns=columns_to_drop)\n",
    "\n",
    "merged_df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Unneccessary columns in the test dataset\n",
    "test_df.drop('id', axis=1, inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Categorization Based on Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_families = merged_df_copy['family'].unique()\n",
    "unique_families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the category lists for each product category\n",
    "food_families = ['BEVERAGES', 'BREAD/BAKERY', 'FROZEN FOODS', 'MEATS', 'PREPARED FOODS', 'DELI','PRODUCE', 'DAIRY','POULTRY','EGGS','SEAFOOD']\n",
    "home_families = ['HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES']\n",
    "clothing_families = ['LINGERIE', 'LADYSWARE']\n",
    "grocery_families = ['GROCERY I', 'GROCERY II']\n",
    "stationery_families = ['BOOKS', 'MAGAZINES','SCHOOL AND OFFICE SUPPLIES']\n",
    "cleaning_families = ['HOME CARE', 'BABY CARE','PERSONAL CARE']\n",
    "hardware_families = ['PLAYERS AND ELECTRONICS','HARDWARE']\n",
    "\n",
    "# Categorize the 'family' column based on the product categories\n",
    "merged_df_copy['family'] = np.where(merged_df_copy['family'].isin(food_families), 'FOODS', merged_df_copy['family'])\n",
    "merged_df_copy['family'] = np.where(merged_df_copy['family'].isin(home_families), 'HOME', merged_df_copy['family'])\n",
    "merged_df_copy['family'] = np.where(merged_df_copy['family'].isin(clothing_families), 'CLOTHING', merged_df_copy['family'])\n",
    "merged_df_copy['family'] = np.where(merged_df_copy['family'].isin(grocery_families), 'GROCERY', merged_df_copy['family'])\n",
    "merged_df_copy['family'] = np.where(merged_df_copy['family'].isin(stationery_families), 'STATIONERY', merged_df_copy['family'])\n",
    "merged_df_copy['family'] = np.where(merged_df_copy['family'].isin(cleaning_families), 'CLEANING', merged_df_copy['family'])\n",
    "merged_df_copy['family'] = np.where(merged_df_copy['family'].isin(hardware_families), 'HARDWARE', merged_df_copy['family'])\n",
    "\n",
    "# Print the updated DataFrame\n",
    "merged_df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Numeric Variables (Min-Max Scaling)\n",
    "# create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# select numerical columns\n",
    "num_cols = ['sales', 'transactions', 'dcoilwtico']\n",
    "\n",
    "# fit and transform the numerical columns\n",
    "merged_df_copy[num_cols] = scaler.fit_transform(merged_df_copy[num_cols])\n",
    "\n",
    "# Display the updated dataframe\n",
    "merged_df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding The Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical columns to encode\n",
    "categorical_columns = [\"family\", \"city\", \"holiday_type\"]\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "one_hot_encoded_data = encoder.fit_transform(merged_df_copy[categorical_columns])\n",
    "\n",
    "# Create column names for the one-hot encoded data\n",
    "column_names = encoder.get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Convert the one-hot encoded data to a DataFrame\n",
    "merged_df_encoded = pd.DataFrame(one_hot_encoded_data.toarray(), columns=column_names)\n",
    "\n",
    "# Concatenate the original dataframe with the one-hot encoded data\n",
    "merged_df_encoded = pd.concat([merged_df_copy, merged_df_encoded], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "merged_df_encoded.drop(categorical_columns, axis=1, inplace=True)\n",
    "\n",
    "# Print the head of the encoded DataFrame\n",
    "merged_df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = merged_df_encoded.loc[merged_df_encoded['year'].isin([2013, 2014, 2015, 2016])]\n",
    "eval_set = merged_df_encoded.loc[merged_df_encoded['year'] == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable and features for training and testing\n",
    "X_train = train_set.drop('sales', axis=1)\n",
    "y_train = train_set['sales'] \n",
    "\n",
    "\n",
    "X_eval = eval_set.drop('sales', axis=1)  \n",
    "y_eval = eval_set['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results dataframe\n",
    "results_df = pd.DataFrame(columns=['Model', 'RMSLE', 'RMSE', 'MSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_predictions = lr_model.predict(X_eval)\n",
    "\n",
    "# Calculate metrics\n",
    "lr_mse = mean_squared_error(y_eval, lr_predictions)\n",
    "lr_mae = mean_absolute_error(y_eval, lr_predictions)\n",
    "lr_rmsle = np.sqrt(mean_squared_log_error(y_eval, lr_predictions))  # Use mean_squared_log_error for RMSLE\n",
    "\n",
    "# Create a DataFrame to store results for Linear Regression\n",
    "results_lr = pd.DataFrame({'Model': ['Linear Regression'],  # Update the model name\n",
    "                            'RMSLE': [lr_rmsle],\n",
    "                            'RMSE': [np.sqrt(lr_mse)],\n",
    "                            'MSE': [lr_mse],\n",
    "                            'MAE': [lr_mae]}).round(2)\n",
    "\n",
    "# Print the results_lr dataframe\n",
    "print(results_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2. Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression Model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_eval)\n",
    "\n",
    "# Calculate metrics\n",
    "rf_mse = mean_squared_error(y_eval, rf_predictions)\n",
    "rf_mae = mean_absolute_error(y_eval, rf_predictions)\n",
    "rf_rmsle = np.sqrt(mean_squared_log_error(y_eval, rf_predictions))  # Use mean_squared_log_error for RMSLE\n",
    "\n",
    "# Create a DataFrame to store results for Random Forest\n",
    "results_rf = pd.DataFrame({'Model': ['Random Forest'],\n",
    "                            'RMSLE': [rf_rmsle],\n",
    "                            'RMSE': [np.sqrt(rf_mse)],\n",
    "                            'MSE': [rf_mse],\n",
    "                            'MAE': [rf_mae]}).round(2)\n",
    "\n",
    "# Print the results_rf dataframe\n",
    "print(results_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3. Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regression Model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_predictions = gb_model.predict(X_eval)\n",
    "\n",
    "# Calculate metrics\n",
    "gb_mse = mean_squared_error(y_eval, gb_predictions)\n",
    "gb_mae = mean_absolute_error(y_eval, gb_predictions)\n",
    "gb_rmsle = np.sqrt(mean_squared_log_error(y_eval, gb_predictions))  # Use mean_squared_log_error for RMSLE\n",
    "\n",
    "# Create a DataFrame to store results for Gradient Boosting\n",
    "results_gb = pd.DataFrame({'Model': ['Gradient Boosting'],\n",
    "                            'RMSLE': [gb_rmsle],\n",
    "                            'RMSE': [np.sqrt(gb_mse)],\n",
    "                            'MSE': [gb_mse],\n",
    "                            'MAE': [gb_mae]}).round(2)\n",
    "\n",
    "# Print the results_gb dataframe\n",
    "print(results_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4. ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Model\n",
    "p = 1\n",
    "d = 0 \n",
    "q = 0  # p and q are equal to zero as data is already stationary\n",
    "\n",
    "# Create an instance of the ARIMA model\n",
    "arima_model = ARIMA(y_train, order=(p, d, q))\n",
    "\n",
    "# Fit the model to the training data\n",
    "arima_model_fit = arima_model.fit()\n",
    "\n",
    "# Make predictions on the evaluation data\n",
    "arima_predictions = arima_model_fit.predict(start=len(y_train), end=len(y_train) + len(X_eval) - 1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "arima_mse = mean_squared_error(y_eval, arima_predictions)\n",
    "arima_rmse = np.sqrt(arima_mse)\n",
    "\n",
    "# Apply the absolute value function to y_eval to remove negative signs\n",
    "y_eval_abs = abs(y_eval)\n",
    "arima_predictions_abs = abs(arima_predictions)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "arima_rmsle = np.sqrt(mean_squared_log_error(y_eval_abs, arima_predictions_abs))\n",
    "\n",
    "# Create a DataFrame to store results for ARIMA\n",
    "results_arima = pd.DataFrame({'Model': ['ARIMA'],\n",
    "                            'RMSLE': [arima_rmsle],\n",
    "                            'RMSE': [np.sqrt(arima_mse)],\n",
    "                            'MSE': [arima_mse],\n",
    "                            'MAE': [arima_mae]}).round(2)\n",
    "\n",
    "# Print the results_arima dataframe\n",
    "results_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5. SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the order and seasonal order parameters\n",
    "P = 0  # Seasonal autoregressive order\n",
    "D = 0  # Seasonal differencing order\n",
    "Q = 0  # Seasonal moving average order\n",
    "m = 0  # Number of time steps in each season\n",
    "\n",
    "# Create an instance of the SARIMA model\n",
    "sarima_model = SARIMAX(endog=y_train, exog=X_train, order=(p, d, q), seasonal_order=(P, D, Q, m))\n",
    "\n",
    "# Fit the model to the training data\n",
    "sarima_fit = sarima_model.fit()\n",
    "\n",
    "# Make predictions on the evaluation data\n",
    "sarima_predictions = sarima_fit.forecast(steps=len(y_eval), exog=X_eval)\n",
    "\n",
    "# Calculate metrics\n",
    "sarima_mse = mean_squared_error(y_eval, sarima_predictions)\n",
    "sarima_rmse = np.sqrt(sarima_mse)\n",
    "sarima_mae = mean_absolute_error(y_eval, sarima_predictions)\n",
    "sarima_rmsle = np.sqrt(mean_squared_error(np.log1p(y_eval), np.log1p(sarima_predictions)))\n",
    "\n",
    "# Create a DataFrame to store results for SARIMA\n",
    "results_sarima = pd.DataFrame({'Model': ['SARIMA'],\n",
    "                                'RMSLE': [sarima_rmsle],\n",
    "                                'RMSE': [sarima_rmse],\n",
    "                                'MSE': [sarima_mse],\n",
    "                                'MAE': [sarima_mae]}).round(2)\n",
    "\n",
    "# Print the results_sarima dataframe\n",
    "results_sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all results to the results dataframe\n",
    "results_df = results_df.append(results_lr)\n",
    "results_df = results_df.append(results_rf)\n",
    "results_df = results_df.append(results_arima)\n",
    "results_df = results_df.append(results_arima)\n",
    "\n",
    "# Print the Final Results dataframe\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
